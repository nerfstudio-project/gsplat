{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e8155f-06ec-4902-88dc-0834a1b0de5e",
   "metadata": {},
   "source": [
    "## Stable Diffusion Feature Extraction\n",
    "\n",
    "The following code is based on the official code of DIFT (https://github.com/Tsingularity/dift). The final part saves the extracted features to a folder named `SD` under the specified scene's folder. Please make sure to install the dependencies in requirements.txt under examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9ba34b-0e9e-4396-8f22-2f7c2ee9d921",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gsplat2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from diffusers.models.unets.unet_2d_condition import UNet2DConditionModel\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "import PIL\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MyUNet2DConditionModel(UNet2DConditionModel):\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "        up_ft_indices,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        deform = None,\n",
    "        class_labels: Optional[torch.Tensor] = None,\n",
    "        timestep_cond: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor\n",
    "            timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps\n",
    "            encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states\n",
    "            cross_attention_kwargs (`dict`, *optional*):\n",
    "                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under\n",
    "                `self.processor` in\n",
    "                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
    "        \"\"\"\n",
    "        # By default samples have to be AT least a multiple of the overall upsampling factor.\n",
    "        # The overall upsampling factor is equal to 2 ** (# num of upsampling layears).\n",
    "        # However, the upsampling interpolation output size can be forced to fit any upsampling size\n",
    "        # on the fly if necessary.\n",
    "        default_overall_up_factor = 2**self.num_upsamplers\n",
    "\n",
    "        # upsample size should be forwarded when sample is not a multiple of `default_overall_up_factor`\n",
    "        forward_upsample_size = False\n",
    "        upsample_size = None\n",
    "\n",
    "        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):\n",
    "            # logger.info(\"Forward upsample size to force interpolation output size.\")\n",
    "            forward_upsample_size = True\n",
    "\n",
    "        # prepare attention_mask\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = (1 - attention_mask.to(sample.dtype)) * -10000.0\n",
    "            attention_mask = attention_mask.unsqueeze(1)\n",
    "\n",
    "        # 0. center input if necessary\n",
    "        if self.config.center_input_sample:\n",
    "            sample = 2 * sample - 1.0\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps.expand(sample.shape[0])\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # timesteps does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=self.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb, timestep_cond)\n",
    "\n",
    "        if self.class_embedding is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\"class_labels should be provided when num_class_embeds > 0\")\n",
    "\n",
    "            if self.config.class_embed_type == \"timestep\":\n",
    "                class_labels = self.time_proj(class_labels)\n",
    "\n",
    "            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)\n",
    "            emb = emb + class_emb\n",
    "\n",
    "        # 2. pre-process\n",
    "        sample = self.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.down_blocks:\n",
    "            if hasattr(downsample_block, \"has_cross_attention\") and downsample_block.has_cross_attention:\n",
    "                sample, res_samples = downsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                )\n",
    "            else:\n",
    "                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        if self.mid_block is not None:\n",
    "            sample = self.mid_block(\n",
    "                sample,\n",
    "                emb,\n",
    "                encoder_hidden_states=encoder_hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                cross_attention_kwargs=cross_attention_kwargs,\n",
    "            )\n",
    "\n",
    "        # 5. up\n",
    "        up_ft = {}\n",
    "        for i, upsample_block in enumerate(self.up_blocks):\n",
    "\n",
    "            is_final_block = i == len(self.up_blocks) - 1\n",
    "\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            # if we have not reached the final block and need to forward the\n",
    "            # upsample size, we do it here\n",
    "            if not is_final_block and forward_upsample_size:\n",
    "                upsample_size = down_block_res_samples[-1].shape[2:]\n",
    "\n",
    "            if hasattr(upsample_block, \"has_cross_attention\") and upsample_block.has_cross_attention:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample,\n",
    "                    temb=emb,\n",
    "                    res_hidden_states_tuple=res_samples,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    upsample_size=upsample_size,\n",
    "                    attention_mask=attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                sample = upsample_block(\n",
    "                    hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size\n",
    "                )\n",
    "\n",
    "            if i in up_ft_indices:\n",
    "                up_ft[i] = sample.detach()\n",
    "                if deform is not None:\n",
    "                    dot = sample.permute(0,2,3,1).reshape((-1,sample.shape[1])) @ deform.unsqueeze(1)\n",
    "                    dot = dot.reshape((sample.shape[0], 1, sample.shape[-2], sample.shape[-1]))\n",
    "                    dot = dot/torch.norm(sample,dim=1, keepdim=True)\n",
    "                    sample = (1+dot)*sample\n",
    "\n",
    "        # 6. post-process\n",
    "        if self.conv_norm_out:\n",
    "            sample = self.conv_norm_out(sample)\n",
    "            sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "\n",
    "        output = {}\n",
    "        output['up_ft'] = up_ft\n",
    "        return output, sample\n",
    "\n",
    "class OneStepSDPipeline(StableDiffusionPipeline):\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        img_tensor,\n",
    "        t,\n",
    "        up_ft_indices,\n",
    "        negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,\n",
    "        callback_steps: int = 1,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        deform = None,\n",
    "        noise = None\n",
    "    ):\n",
    "\n",
    "        device = self._execution_device\n",
    "        latents = self.vae.encode(img_tensor).latent_dist.sample() * self.vae.config.scaling_factor\n",
    "        t = torch.tensor(t, dtype=torch.long, device=device)\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(latents).to(device)\n",
    "        latents_noisy = self.scheduler.add_noise(latents, noise, t)\n",
    "        unet_output, noise_pred = self.unet(latents_noisy,\n",
    "                               t,\n",
    "                               up_ft_indices,\n",
    "                               encoder_hidden_states=prompt_embeds,\n",
    "                               cross_attention_kwargs=cross_attention_kwargs,\n",
    "                               deform=deform          )\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents_clean = self.scheduler.step(noise_pred, t, latents_noisy).pred_original_sample\n",
    "        # print(latents_clean.shape, noise_pred.shape)\n",
    "\n",
    "        # scale and decode the image latents with vae\n",
    "        latents_clean = 1 / self.vae.config.scaling_factor * latents_clean\n",
    "        image = self.vae.decode(latents_clean).sample\n",
    "\n",
    "        return unet_output, image\n",
    "\n",
    "\n",
    "class SDFeaturizer:\n",
    "    def __init__(self, sd_id='stabilityai/stable-diffusion-2-1', index=1):\n",
    "        unet = MyUNet2DConditionModel.from_pretrained(sd_id, subfolder=\"unet\", use_safetensors=False)\n",
    "        onestep_pipe = OneStepSDPipeline.from_pretrained(sd_id, unet=unet, safety_checker=None, use_safetensors=False)\n",
    "        onestep_pipe.scheduler = DDIMScheduler.from_pretrained(sd_id, subfolder=\"scheduler\", use_safetensors=False)\n",
    "        onestep_pipe.scheduler.set_timesteps(50)\n",
    "        gc.collect()\n",
    "        onestep_pipe = onestep_pipe.to(\"cuda\")\n",
    "        onestep_pipe.enable_attention_slicing()\n",
    "        self.pipe = onestep_pipe\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self,\n",
    "                img_tensor, # single image, [1,c,h,w]\n",
    "                prompt,\n",
    "                deform=None,\n",
    "                t=261,\n",
    "                up_ft_index=[1],\n",
    "                ensemble_size=8, noise=None):\n",
    "\n",
    "        img_tensor = img_tensor.repeat(ensemble_size, 1, 1, 1).cuda() # ensem, c, h, w\n",
    "\n",
    "        prompt_embeds = self.pipe.encode_prompt(\n",
    "            prompt=prompt,\n",
    "            device='cuda',\n",
    "            num_images_per_prompt=1,\n",
    "            do_classifier_free_guidance=False)[0] # [1, 77, dim]\n",
    "        prompt_embeds = prompt_embeds.repeat(ensemble_size, 1, 1)\n",
    "        unet_ft_all, image = self.pipe(\n",
    "            img_tensor=img_tensor,\n",
    "            t=t,\n",
    "            up_ft_indices=up_ft_index,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            deform=deform, noise=noise)\n",
    "        fts = []\n",
    "        mx_shape = 0,0\n",
    "        for i in up_ft_index:\n",
    "            unet_ft = unet_ft_all['up_ft'][i] # ensem, c, h, w\n",
    "            unet_ft = unet_ft.mean(0, keepdim=True) # 1,c,h,w\n",
    "            mx_shape = max(mx_shape[0],unet_ft.shape[-2]), max(mx_shape[0],unet_ft.shape[-1])\n",
    "            fts += [unet_ft]\n",
    "        fts_resized = []\n",
    "        for i in range(len(up_ft_index)):\n",
    "            fts_resized += F.interpolate(fts[i], size=(mx_shape[0], mx_shape[1]), mode='bilinear')\n",
    "\n",
    "        unet_ft_all = torch.cat(fts_resized,dim=0)  #n,c,h,w\n",
    "        return unet_ft_all, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "448f5597-e119-4eb7-9d23-b14bbab4a958",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "/opt/conda/envs/gsplat2/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/gsplat2/lib/python3.9/site-packages/diffusers/models/modeling_utils.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Fetching 11 files: 100%|███████████████████████████████████████| 11/11 [00:20<00:00,  1.84s/it]\n",
      "Loading pipeline components...: 100%|████████████████████████████| 6/6 [00:01<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# This might take a while if `accelerate` is not installed.\n",
    "dift = SDFeaturizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e747e47-52c6-4f47-bf7e-8b9ba8a097e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file: 2clutter59.npy\n",
      "Saving file: 1extra55.npy\n",
      "Saving file: 1extra164.npy\n",
      "Saving file: 1extra113.npy\n",
      "Saving file: 1extra183.npy\n",
      "Saving file: 1extra190.npy\n",
      "Saving file: 2clutter11.npy\n",
      "Saving file: 0clean22.npy\n",
      "Saving file: 1extra193.npy\n",
      "Saving file: 2clutter58.npy\n",
      "Saving file: 1extra117.npy\n",
      "Saving file: 1extra43.npy\n",
      "Saving file: 0clean83.npy\n",
      "Saving file: 1extra44.npy\n",
      "Saving file: 0clean107.npy\n",
      "Saving file: 2clutter44.npy\n",
      "Saving file: 2clutter93.npy\n",
      "Saving file: 1extra139.npy\n",
      "Saving file: 1extra125.npy\n",
      "Saving file: 2clutter102.npy\n",
      "Saving file: 2clutter47.npy\n",
      "Saving file: 1extra132.npy\n",
      "Saving file: 2clutter25.npy\n",
      "Saving file: 1extra57.npy\n",
      "Saving file: 1extra142.npy\n",
      "Saving file: 0clean3.npy\n",
      "Saving file: 0clean98.npy\n",
      "Saving file: 2clutter73.npy\n",
      "Saving file: 1extra167.npy\n",
      "Saving file: 2clutter4.npy\n",
      "Saving file: 2clutter37.npy\n",
      "Saving file: 0clean42.npy\n",
      "Saving file: 0clean9.npy\n",
      "Saving file: 0clean64.npy\n",
      "Saving file: 1extra75.npy\n",
      "Saving file: 0clean84.npy\n",
      "Saving file: 0clean108.npy\n",
      "Saving file: 2clutter108.npy\n",
      "Saving file: 2clutter68.npy\n",
      "Saving file: 1extra163.npy\n",
      "Saving file: 2clutter107.npy\n",
      "Saving file: 0clean106.npy\n",
      "Saving file: 2clutter86.npy\n",
      "Saving file: 1extra137.npy\n",
      "Saving file: 1extra141.npy\n",
      "Saving file: 0clean46.npy\n",
      "Saving file: 1extra77.npy\n",
      "Saving file: 2clutter62.npy\n",
      "Saving file: 1extra149.npy\n",
      "Saving file: 1extra52.npy\n",
      "Saving file: 1extra107.npy\n",
      "Saving file: 1extra147.npy\n",
      "Saving file: 1extra53.npy\n",
      "Saving file: 0clean60.npy\n",
      "Saving file: 0clean19.npy\n",
      "Saving file: 1extra112.npy\n",
      "Saving file: 0clean33.npy\n",
      "Saving file: 2clutter55.npy\n",
      "Saving file: 0clean6.npy\n",
      "Saving file: 1extra24.npy\n",
      "Saving file: 1extra111.npy\n",
      "Saving file: 1extra30.npy\n",
      "Saving file: 0clean74.npy\n",
      "Saving file: 1extra166.npy\n",
      "Saving file: 0clean75.npy\n",
      "Saving file: 1extra8.npy\n",
      "Saving file: 2clutter56.npy\n",
      "Saving file: 1extra32.npy\n",
      "Saving file: 2clutter72.npy\n",
      "Saving file: 1extra160.npy\n",
      "Saving file: 0clean61.npy\n",
      "Saving file: 1extra15.npy\n",
      "Saving file: 1extra42.npy\n",
      "Saving file: 1extra129.npy\n",
      "Saving file: 2clutter67.npy\n",
      "Saving file: 1extra192.npy\n",
      "Saving file: 2clutter109.npy\n",
      "Saving file: 1extra68.npy\n",
      "Saving file: 1extra22.npy\n",
      "Saving file: 1extra119.npy\n",
      "Saving file: 0clean2.npy\n",
      "Saving file: 1extra72.npy\n",
      "Saving file: 0clean91.npy\n",
      "Saving file: 1extra54.npy\n",
      "Saving file: 1extra99.npy\n",
      "Saving file: 2clutter36.npy\n",
      "Saving file: 0clean68.npy\n",
      "Saving file: 1extra169.npy\n",
      "Saving file: 1extra73.npy\n",
      "Saving file: 2clutter95.npy\n",
      "Saving file: 2clutter10.npy\n",
      "Saving file: 1extra127.npy\n",
      "Saving file: 2clutter74.npy\n",
      "Saving file: 1extra130.npy\n",
      "Saving file: 1extra70.npy\n",
      "Saving file: 2clutter99.npy\n",
      "Saving file: 0clean102.npy\n",
      "Saving file: 2clutter7.npy\n",
      "Saving file: 2clutter46.npy\n",
      "Saving file: 1extra187.npy\n",
      "Saving file: 1extra148.npy\n",
      "Saving file: 2clutter6.npy\n",
      "Saving file: 0clean57.npy\n",
      "Saving file: 1extra154.npy\n",
      "Saving file: 1extra188.npy\n",
      "Saving file: 0clean8.npy\n",
      "Saving file: 1extra48.npy\n",
      "Saving file: 2clutter12.npy\n",
      "Saving file: 1extra102.npy\n",
      "Saving file: 0clean105.npy\n",
      "Saving file: 0clean43.npy\n",
      "Saving file: 1extra2.npy\n",
      "Saving file: 1extra121.npy\n",
      "Saving file: 2clutter103.npy\n",
      "Saving file: 1extra89.npy\n",
      "Saving file: 1extra27.npy\n",
      "Saving file: 1extra19.npy\n",
      "Saving file: 2clutter33.npy\n",
      "Saving file: 2clutter82.npy\n",
      "Saving file: 1extra1.npy\n",
      "Saving file: 1extra186.npy\n",
      "Saving file: 1extra159.npy\n",
      "Saving file: 1extra105.npy\n",
      "Saving file: 2clutter31.npy\n",
      "Saving file: 2clutter24.npy\n",
      "Saving file: 0clean66.npy\n",
      "Saving file: 2clutter91.npy\n",
      "Saving file: 0clean103.npy\n",
      "Saving file: 1extra69.npy\n",
      "Saving file: 0clean96.npy\n",
      "Saving file: 1extra61.npy\n",
      "Saving file: 0clean54.npy\n",
      "Saving file: 1extra9.npy\n",
      "Saving file: 0clean82.npy\n",
      "Saving file: 2clutter38.npy\n",
      "Saving file: 1extra153.npy\n",
      "Saving file: 2clutter81.npy\n",
      "Saving file: 1extra138.npy\n",
      "Saving file: 1extra36.npy\n",
      "Saving file: 0clean50.npy\n",
      "Saving file: 2clutter69.npy\n",
      "Saving file: 1extra18.npy\n",
      "Saving file: 1extra4.npy\n",
      "Saving file: 2clutter51.npy\n",
      "Saving file: 1extra16.npy\n",
      "Saving file: 0clean88.npy\n",
      "Saving file: 0clean12.npy\n",
      "Saving file: 2clutter100.npy\n",
      "Saving file: 1extra116.npy\n",
      "Saving file: 1extra168.npy\n",
      "Saving file: 1extra198.npy\n",
      "Saving file: 1extra86.npy\n",
      "Saving file: 0clean72.npy\n",
      "Saving file: 2clutter22.npy\n",
      "Saving file: 0clean53.npy\n",
      "Saving file: 1extra170.npy\n",
      "Saving file: 1extra150.npy\n",
      "Saving file: 1extra21.npy\n",
      "Saving file: 1extra40.npy\n",
      "Saving file: 0clean90.npy\n",
      "Saving file: 0clean93.npy\n",
      "Saving file: 0clean28.npy\n",
      "Saving file: 1extra33.npy\n",
      "Saving file: 0clean16.npy\n",
      "Saving file: 0clean47.npy\n",
      "Saving file: 2clutter57.npy\n",
      "Saving file: 1extra180.npy\n",
      "Saving file: 1extra90.npy\n",
      "Saving file: 1extra145.npy\n",
      "Saving file: 2clutter18.npy\n",
      "Saving file: 2clutter66.npy\n",
      "Saving file: 2clutter106.npy\n",
      "Saving file: 0clean26.npy\n",
      "Saving file: 2clutter27.npy\n",
      "Saving file: 1extra14.npy\n",
      "Saving file: 1extra140.npy\n",
      "Saving file: 0clean77.npy\n",
      "Saving file: 1extra71.npy\n",
      "Saving file: 2clutter52.npy\n",
      "Saving file: 0clean38.npy\n",
      "Saving file: 1extra26.npy\n",
      "Saving file: 1extra13.npy\n",
      "Saving file: 0clean37.npy\n",
      "Saving file: 1extra173.npy\n",
      "Saving file: 0clean29.npy\n",
      "Saving file: 2clutter105.npy\n",
      "Saving file: 0clean15.npy\n",
      "Saving file: 0clean17.npy\n",
      "Saving file: 0clean56.npy\n",
      "Saving file: 1extra34.npy\n",
      "Saving file: 1extra106.npy\n",
      "Saving file: 1extra178.npy\n",
      "Saving file: 1extra194.npy\n",
      "Saving file: 1extra100.npy\n",
      "Saving file: 0clean4.npy\n",
      "Saving file: 1extra63.npy\n",
      "Saving file: 1extra131.npy\n",
      "Saving file: 0clean87.npy\n",
      "Saving file: 1extra91.npy\n",
      "Saving file: 1extra20.npy\n",
      "Saving file: 1extra201.npy\n",
      "Saving file: 0clean73.npy\n",
      "Saving file: 1extra199.npy\n",
      "Saving file: 1extra162.npy\n",
      "Saving file: 2clutter84.npy\n",
      "Saving file: 0clean27.npy\n",
      "Saving file: 1extra94.npy\n",
      "Saving file: 2clutter85.npy\n",
      "Saving file: 1extra109.npy\n",
      "Saving file: 0clean63.npy\n",
      "Saving file: 0clean65.npy\n",
      "Saving file: 2clutter101.npy\n",
      "Saving file: 1extra76.npy\n",
      "Saving file: 1extra58.npy\n",
      "Saving file: 1extra17.npy\n",
      "Saving file: 1extra23.npy\n",
      "Saving file: 0clean44.npy\n",
      "Saving file: 1extra46.npy\n",
      "Saving file: 1extra123.npy\n",
      "Saving file: 1extra29.npy\n",
      "Saving file: 1extra6.npy\n",
      "Saving file: 0clean18.npy\n",
      "Saving file: 1extra87.npy\n",
      "Saving file: 1extra31.npy\n",
      "Saving file: 0clean59.npy\n",
      "Saving file: 1extra83.npy\n",
      "Saving file: 1extra165.npy\n",
      "Saving file: 1extra151.npy\n",
      "Saving file: 1extra104.npy\n",
      "Saving file: 1extra133.npy\n",
      "Saving file: 1extra39.npy\n",
      "Saving file: 1extra101.npy\n",
      "Saving file: 0clean34.npy\n",
      "Saving file: 1extra51.npy\n",
      "Saving file: 1extra191.npy\n",
      "Saving file: 2clutter41.npy\n",
      "Saving file: 2clutter3.npy\n",
      "Saving file: 1extra144.npy\n",
      "Saving file: 1extra79.npy\n",
      "Saving file: 0clean35.npy\n",
      "Saving file: 2clutter83.npy\n",
      "Saving file: 0clean14.npy\n",
      "Saving file: 2clutter97.npy\n",
      "Saving file: 2clutter78.npy\n",
      "Saving file: 1extra181.npy\n",
      "Saving file: 2clutter13.npy\n",
      "Saving file: 2clutter29.npy\n",
      "Saving file: 0clean67.npy\n",
      "Saving file: 2clutter16.npy\n",
      "Saving file: 1extra161.npy\n",
      "Saving file: 2clutter92.npy\n",
      "Saving file: 1extra122.npy\n",
      "Saving file: 0clean32.npy\n",
      "Saving file: 1extra124.npy\n",
      "Saving file: 1extra126.npy\n",
      "Saving file: 2clutter88.npy\n",
      "Saving file: 1extra97.npy\n",
      "Saving file: 0clean58.npy\n",
      "Saving file: 0clean7.npy\n",
      "Saving file: 0clean99.npy\n",
      "Saving file: 2clutter45.npy\n",
      "Saving file: 2clutter80.npy\n",
      "Saving file: 1extra82.npy\n",
      "Saving file: 0clean80.npy\n",
      "Saving file: 1extra28.npy\n",
      "Saving file: 2clutter14.npy\n",
      "Saving file: 2clutter9.npy\n",
      "Saving file: 2clutter15.npy\n",
      "Saving file: 1extra196.npy\n",
      "Saving file: 2clutter30.npy\n",
      "Saving file: 1extra84.npy\n",
      "Saving file: 2clutter71.npy\n",
      "Saving file: 0clean24.npy\n",
      "Saving file: 0clean5.npy\n",
      "Saving file: 2clutter8.npy\n",
      "Saving file: 2clutter89.npy\n",
      "Saving file: 2clutter54.npy\n",
      "Saving file: 0clean62.npy\n",
      "Saving file: 1extra146.npy\n",
      "Saving file: 0clean48.npy\n",
      "Saving file: 1extra182.npy\n",
      "Saving file: 1extra78.npy\n",
      "Saving file: 1extra66.npy\n",
      "Saving file: 1extra60.npy\n",
      "Saving file: 2clutter19.npy\n",
      "Saving file: 2clutter42.npy\n",
      "Saving file: 1extra10.npy\n",
      "Saving file: 2clutter35.npy\n",
      "Saving file: 1extra110.npy\n",
      "Saving file: 1extra80.npy\n",
      "Saving file: 2clutter61.npy\n",
      "Saving file: 1extra67.npy\n",
      "Saving file: 1extra85.npy\n",
      "Saving file: 1extra197.npy\n",
      "Saving file: 0clean76.npy\n",
      "Saving file: 0clean10.npy\n",
      "Saving file: 1extra12.npy\n",
      "Saving file: 2clutter21.npy\n",
      "Saving file: 1extra59.npy\n",
      "Saving file: 0clean70.npy\n",
      "Saving file: 2clutter60.npy\n",
      "Saving file: 1extra176.npy\n",
      "Saving file: 0clean45.npy\n",
      "Saving file: 2clutter70.npy\n",
      "Saving file: 0clean85.npy\n",
      "Saving file: 0clean41.npy\n",
      "Saving file: 1extra41.npy\n",
      "Saving file: 1extra47.npy\n",
      "Saving file: 2clutter49.npy\n",
      "Saving file: 0clean109.npy\n",
      "Saving file: 1extra103.npy\n",
      "Saving file: 1extra115.npy\n",
      "Saving file: 1extra95.npy\n",
      "Saving file: 1extra134.npy\n",
      "Saving file: 1extra135.npy\n",
      "Saving file: 1extra177.npy\n",
      "Saving file: 0clean69.npy\n",
      "Saving file: 1extra158.npy\n",
      "Saving file: 2clutter64.npy\n",
      "Saving file: 0clean51.npy\n",
      "Saving file: 1extra189.npy\n",
      "Saving file: 2clutter2.npy\n",
      "Saving file: 1extra92.npy\n",
      "Saving file: 0clean11.npy\n",
      "Saving file: 0clean97.npy\n",
      "Saving file: 0clean94.npy\n",
      "Saving file: 1extra195.npy\n",
      "Saving file: 0clean81.npy\n",
      "Saving file: 1extra5.npy\n",
      "Saving file: 1extra7.npy\n",
      "Saving file: 2clutter20.npy\n",
      "Saving file: 1extra171.npy\n",
      "Saving file: 2clutter65.npy\n",
      "Saving file: 2clutter79.npy\n",
      "Saving file: 1extra45.npy\n",
      "Saving file: 0clean86.npy\n",
      "Saving file: 1extra35.npy\n",
      "Saving file: 2clutter104.npy\n",
      "Saving file: 0clean13.npy\n",
      "Saving file: 0clean79.npy\n",
      "Saving file: 1extra128.npy\n",
      "Saving file: 1extra184.npy\n",
      "Saving file: 1extra185.npy\n",
      "Saving file: 2clutter63.npy\n",
      "Saving file: 0clean92.npy\n",
      "Saving file: 2clutter43.npy\n",
      "Saving file: 2clutter50.npy\n",
      "Saving file: 1extra152.npy\n",
      "Saving file: 1extra64.npy\n",
      "Saving file: 0clean20.npy\n",
      "Saving file: 1extra118.npy\n",
      "Saving file: 1extra38.npy\n",
      "Saving file: 2clutter34.npy\n",
      "Saving file: 2clutter5.npy\n",
      "Saving file: 1extra98.npy\n",
      "Saving file: 0clean1.npy\n",
      "Saving file: 0clean36.npy\n",
      "Saving file: 1extra172.npy\n",
      "Saving file: 2clutter32.npy\n",
      "Saving file: 1extra157.npy\n",
      "Saving file: 0clean49.npy\n",
      "Saving file: 1extra179.npy\n",
      "Saving file: 1extra156.npy\n",
      "Saving file: 1extra108.npy\n",
      "Saving file: 0clean52.npy\n",
      "Saving file: 2clutter23.npy\n",
      "Saving file: 2clutter90.npy\n",
      "Saving file: 0clean71.npy\n",
      "Saving file: 2clutter17.npy\n",
      "Saving file: 0clean25.npy\n",
      "Saving file: 1extra120.npy\n",
      "Saving file: 1extra50.npy\n",
      "Saving file: 0clean31.npy\n",
      "Saving file: 1extra155.npy\n",
      "Saving file: 2clutter98.npy\n",
      "Saving file: 1extra200.npy\n",
      "Saving file: 1extra37.npy\n",
      "Saving file: 0clean39.npy\n",
      "Saving file: 2clutter39.npy\n",
      "Saving file: 1extra25.npy\n",
      "Saving file: 0clean100.npy\n",
      "Saving file: 2clutter94.npy\n",
      "Saving file: 1extra11.npy\n",
      "Saving file: 0clean30.npy\n",
      "Saving file: 2clutter40.npy\n",
      "Saving file: 0clean89.npy\n",
      "Saving file: 1extra114.npy\n",
      "Saving file: 1extra202.npy\n",
      "Saving file: 1extra3.npy\n",
      "Saving file: 0clean40.npy\n",
      "Saving file: 1extra81.npy\n",
      "Saving file: 1extra56.npy\n",
      "Saving file: 0clean101.npy\n",
      "Saving file: 1extra93.npy\n",
      "Saving file: 0clean55.npy\n",
      "Saving file: 1extra174.npy\n",
      "Saving file: 2clutter77.npy\n",
      "Saving file: 1extra65.npy\n",
      "Saving file: 1extra74.npy\n",
      "Saving file: 1extra88.npy\n",
      "Saving file: 2clutter87.npy\n",
      "Saving file: 2clutter28.npy\n",
      "Saving file: 0clean21.npy\n",
      "Saving file: 2clutter1.npy\n",
      "Saving file: 2clutter76.npy\n",
      "Saving file: 0clean95.npy\n",
      "Saving file: 0clean23.npy\n",
      "Saving file: 2clutter26.npy\n",
      "Saving file: 2clutter96.npy\n",
      "Saving file: 0clean104.npy\n",
      "Saving file: 2clutter48.npy\n",
      "Saving file: 2clutter53.npy\n",
      "Saving file: 2clutter75.npy\n",
      "Saving file: 1extra96.npy\n",
      "Saving file: 0clean78.npy\n",
      "Saving file: 1extra175.npy\n",
      "Saving file: 1extra62.npy\n",
      "Saving file: 1extra143.npy\n",
      "Saving file: 1extra49.npy\n",
      "Saving file: 1extra136.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root = \"../data/spotless/yoda3/\" # @param\n",
    "if not os.path.exists(root+\"SD\"):\n",
    "    os.mkdir(root+\"SD\")\n",
    "filelist = os.listdir(root+\"images\")\n",
    "\n",
    "img_size = 800\n",
    "\n",
    "for filename in filelist:\n",
    "    name = filename.split('.')[0] + '.npy'\n",
    "    if os.path.exists(root+\"SD/\"+name):\n",
    "        continue\n",
    "    img = PIL.Image.open(root+\"images/\"+filename).convert('RGB')\n",
    "    img = img.resize((img_size, img_size))\n",
    "    img_tensor = (torch.tensor(np.array(img)) / 255.0 - 0.5) * 2\n",
    "    img_tensor = img_tensor.permute(2,0,1)\n",
    "    fts, image = dift.forward(img_tensor,\n",
    "                           prompt='',\n",
    "                           ensemble_size=4,\n",
    "                           t = 261,\n",
    "                           up_ft_index=[1,])\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "\n",
    "    ft_np = fts.clone().detach().cpu().numpy()\n",
    "    print(\"Saving file:\",name)\n",
    "    with open(root + \"SD/\" + name, \"wb\") as fout:\n",
    "         np.save(fout, ft_np)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
